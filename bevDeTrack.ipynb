{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import mmcv\n",
    "import torch\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.cnn import fuse_conv_bn\n",
    "from mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n",
    "from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,wrap_fp16_model)\n",
    "\n",
    "import mmdet\n",
    "from mmdet3d.apis import single_gpu_test\n",
    "from mmdet3d.datasets import build_dataloader, build_dataset\n",
    "from mmdet3d.models import build_model\n",
    "from mmdet.apis import multi_gpu_test, set_random_seed\n",
    "from mmdet.datasets import replace_ImageToTensor\n",
    "\n",
    "if mmdet.__version__ > '2.23.0':\n",
    "    # If mmdet version > 2.23.0, setup_multi_processes would be imported and\n",
    "    # used from mmdet instead of mmdet3d.\n",
    "    from mmdet.utils import setup_multi_processes\n",
    "else:\n",
    "    from mmdet3d.utils import setup_multi_processes\n",
    "\n",
    "try:\n",
    "    # If mmdet version > 2.23.0, compat_cfg would be imported and\n",
    "    # used from mmdet instead of mmdet3d.\n",
    "    from mmdet.utils import compat_cfg\n",
    "except ImportError:\n",
    "    from mmdet3d.utils import compat_cfg\n",
    "    \n",
    "import json\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pyquaternion.quaternion import Quaternion\n",
    "from mmdet3d.core.bbox.structures.lidar_box3d import LiDARInstance3DBoxes as LB\n",
    "\n",
    "from nuscenes.utils.data_classes import Box, LidarPointCloud\n",
    "from nuscenes.utils.geometry_utils import view_points, box_in_image, BoxVisibility, transform_matrix\n",
    "from PIL import Image\n",
    "from pyquaternion import Quaternion\n",
    "import pyquaternion\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import glob\n",
    "from math import pi\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "\n",
    "# PoseNet\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from scipy.spatial.transform import Rotation as R\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quaternion_from_euler(e):\n",
    "    \"\"\"\n",
    "    Convert an Euler angle to a quaternion.\n",
    "\n",
    "    Input\n",
    "    :param roll: The roll (rotation around x-axis) angle in radians.\n",
    "    :param pitch: The pitch (rotation around y-axis) angle in radians.\n",
    "    :param yaw: The yaw (rotation around z-axis) angle in radians.\n",
    "\n",
    "    Output\n",
    "    :return qx, qy, qz, qw: The orientation in quaternion [x,y,z,w] format\n",
    "    \"\"\"\n",
    "    roll = e[0]\n",
    "    pitch = e[1]\n",
    "    yaw = e[2]\n",
    "\n",
    "    qx = np.sin(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) - np.cos(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)\n",
    "    qy = np.cos(roll/2) * np.sin(pitch/2) * np.cos(yaw/2) + np.sin(roll/2) * np.cos(pitch/2) * np.sin(yaw/2)\n",
    "    qz = np.cos(roll/2) * np.cos(pitch/2) * np.sin(yaw/2) - np.sin(roll/2) * np.sin(pitch/2) * np.cos(yaw/2)\n",
    "    qw = np.cos(roll/2) * np.cos(pitch/2) * np.cos(yaw/2) + np.sin(roll/2) * np.sin(pitch/2) * np.sin(yaw/2)\n",
    "\n",
    "    return [qw, qx, qy, qz]\n",
    "\n",
    "def euler_from_quaternion(q):\n",
    "    \"\"\"\n",
    "    Convert a quaternion into euler angles (roll, pitch, yaw)\n",
    "    roll is rotation around x in radians (counterclockwise)\n",
    "    pitch is rotation around y in radians (counterclockwise)\n",
    "    yaw is rotation around z in radians (counterclockwise)\n",
    "    \"\"\"\n",
    "    import math\n",
    "    w = q[0]\n",
    "    x = q[1]\n",
    "    y = q[2]\n",
    "    z = q[3]\n",
    "    \n",
    "    t0 = +2.0 * (w * x + y * z)\n",
    "    t1 = +1.0 - 2.0 * (x * x + y * y)\n",
    "    # roll_x = math.atan2(t0, t1) / np.pi * 180 # degrees\n",
    "    roll_x = math.atan2(t0, t1)\n",
    "\n",
    "    t2 = +2.0 * (w * y - z * x)\n",
    "    t2 = +1.0 if t2 > +1.0 else t2\n",
    "    t2 = -1.0 if t2 < -1.0 else t2\n",
    "    # pitch_y = math.asin(t2) / np.pi * 180\n",
    "    pitch_y = math.asin(t2)\n",
    "\n",
    "    t3 = +2.0 * (w * z + x * y)\n",
    "    t4 = +1.0 - 2.0 * (y * y + z * z) \n",
    "    # yaw_z = math.atan2(t3, t4) / np.pi * 180\n",
    "    yaw_z = math.atan2(t3, t4)\n",
    "\n",
    "    return [roll_x, pitch_y, yaw_z] # in radian\n",
    "\n",
    "# https://answers.ros.org/question/388140/converting-a-rotation-matrix-to-quaternions-in-python/\n",
    "def rotationMatrixToQuaternion1(m):\n",
    "    #q0 = qw\n",
    "    t = np.matrix.trace(m)\n",
    "    q = np.asarray([0.0, 0.0, 0.0, 0.0], dtype=np.float64)\n",
    "\n",
    "    if(t > 0):\n",
    "        t = np.sqrt(t + 1)\n",
    "        q[0] = 0.5 * t\n",
    "        t = 0.5/t\n",
    "        q[1] = (m[2,1] - m[1,2]) * t\n",
    "        q[2] = (m[0,2] - m[2,0]) * t\n",
    "        q[3] = (m[1,0] - m[0,1]) * t\n",
    "\n",
    "    else:\n",
    "        i = 0\n",
    "        if (m[1,1] > m[0,0]):\n",
    "            i = 1\n",
    "        if (m[2,2] > m[i,i]):\n",
    "            i = 2\n",
    "        j = (i+1)%3\n",
    "        k = (j+1)%3\n",
    "\n",
    "        t = np.sqrt(m[i,i] - m[j,j] - m[k,k] + 1)\n",
    "        q[i] = 0.5 * t\n",
    "        t = 0.5 / t\n",
    "        q[0] = (m[k,j] - m[j,k]) * t\n",
    "        q[j] = (m[j,i] + m[i,j]) * t\n",
    "        q[k] = (m[k,i] + m[i,k]) * t\n",
    "\n",
    "    return q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tangent Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createProjectGrid(erp_h, erp_w, tangent_h, tangent_w, num_rows, num_cols, phi_centers, fov):\n",
    "    height, width = tangent_h, tangent_w\n",
    "\n",
    "    FOV = fov\n",
    "    FOV = [FOV[0] / 360.0, FOV[1] / 180.0]\n",
    "    FOV = torch.tensor(FOV, dtype=torch.float32)\n",
    "\n",
    "    PI = math.pi\n",
    "    PI_2 = math.pi * 0.5\n",
    "    PI2 = math.pi * 2\n",
    "\n",
    "    yy, xx = torch.meshgrid(torch.linspace(0, 1, height), torch.linspace(0, 1, width))\n",
    "    screen_points = torch.stack([xx.flatten(), yy.flatten()], -1)\n",
    "\n",
    "    num_rows = num_rows\n",
    "    num_cols = num_cols\n",
    "    phi_centers = phi_centers\n",
    "\n",
    "    phi_interval = 180 // num_rows\n",
    "    all_combos = []\n",
    "    erp_mask = []\n",
    "\n",
    "    for i, n_cols in enumerate(num_cols):\n",
    "        for j in np.arange(n_cols): # 0 ~ num_cols.length\n",
    "            theta_interval = 360 / n_cols # 현재 row (위도)에서 쪼개질 경도 (col)의 위치\n",
    "            theta_center = j * theta_interval + theta_interval / 2\n",
    "            center = [theta_center, phi_centers[i]] # 각 tangent image의 center position\n",
    "\n",
    "            # print(str(j) + \" th theta center \" + str(theta_center) + \" phi center \" + str(phi_centers[i]))\n",
    "            \n",
    "            all_combos.append(center)\n",
    "\n",
    "            # 구좌표계에서의 tangent image가 차지하는 영역에 대한 좌표들\n",
    "            up = phi_centers[i] + phi_interval / 2\n",
    "            down = phi_centers[i] - phi_interval / 2\n",
    "            left = theta_center - theta_interval / 2\n",
    "            right = theta_center + theta_interval / 2\n",
    "\n",
    "            # ERP image에서 현재 tangent가 차지하는 영역에 대한 pixel 위치들\n",
    "            up = int((up + 90) / 180 * erp_h)\n",
    "            down = int((down + 90) / 180 * erp_h)\n",
    "            left = int(left / 360 * erp_w)\n",
    "            right = int(right / 360 * erp_w)\n",
    "\n",
    "            # ERP 이미지에서 현재 tangent image 영역에 해당하는 부분에 1로 마스킹\n",
    "            mask = np.zeros((erp_h, erp_w), dtype=int)\n",
    "            mask[down:up, left:right] = 1\n",
    "            erp_mask.append(mask)\n",
    "\n",
    "    all_combos = np.vstack(all_combos)\n",
    "    shifts = np.arange(all_combos.shape[0]) * width\n",
    "    shifts = torch.from_numpy(shifts).float()\n",
    "    erp_mask = np.stack(erp_mask)\n",
    "    erp_mask = torch.from_numpy(erp_mask).float()\n",
    "    n_patch = all_combos.shape[0]\n",
    "    \n",
    "    center_point = torch.from_numpy(all_combos).float()  # -180 to 180, -90 to 90\n",
    "    center_point[:, 0] = (center_point[:, 0]) / 360  #0 to 1\n",
    "    center_point[:, 1] = (center_point[:, 1] + 90) / 180  #0 to 1\n",
    "\n",
    "    cp = center_point * 2 - 1\n",
    "    cp[:, 0] = cp[:, 0] * PI\n",
    "    cp[:, 1] = cp[:, 1] * PI_2\n",
    "    cp = cp.unsqueeze(1)\n",
    "\n",
    "    convertedCoord = screen_points * 2 - 1\n",
    "    convertedCoord[:, 0] = convertedCoord[:, 0] * PI\n",
    "    convertedCoord[:, 1] = convertedCoord[:, 1] * PI_2\n",
    "    convertedCoord = convertedCoord * (torch.ones(screen_points.shape, dtype=torch.float32) * FOV)\n",
    "    convertedCoord = convertedCoord.unsqueeze(0).repeat(cp.shape[0], 1, 1)\n",
    "\n",
    "    x = convertedCoord[:, :, 0]\n",
    "    y = convertedCoord[:, :, 1]\n",
    "\n",
    "    rou = torch.sqrt(x ** 2 + y ** 2)\n",
    "    c = torch.atan(rou)\n",
    "    sin_c = torch.sin(c)\n",
    "    cos_c = torch.cos(c)\n",
    "    lat = torch.asin(cos_c * torch.sin(cp[:, :, 1]) + (y * sin_c * torch.cos(cp[:, :, 1])) / rou)\n",
    "    lon = cp[:, :, 0] + torch.atan2(x * sin_c, rou * torch.cos(cp[:, :, 1]) * cos_c - y * torch.sin(cp[:, :, 1]) * sin_c)\n",
    "    lat_new = lat / PI_2\n",
    "    lon_new = lon / PI\n",
    "    lon_new[lon_new > 1] -= 2\n",
    "    lon_new[lon_new<-1] += 2\n",
    "\n",
    "    lon_new = lon_new.view(1, n_patch, height, width).permute(0, 2, 1, 3).contiguous().view(height, n_patch*width)\n",
    "    lat_new = lat_new.view(1, n_patch, height, width).permute(0, 2, 1, 3).contiguous().view(height, n_patch*width)\n",
    "    \n",
    "    grid = torch.stack([lon_new, lat_new], -1)\n",
    "    grid = grid.unsqueeze(0)\n",
    "\n",
    "    return n_patch, grid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jeholee/anaconda3/envs/omnicv/lib/python3.8/site-packages/mmdet/models/backbones/resnet.py:401: UserWarning: DeprecationWarning: pretrained is deprecated, please use \"init_cfg\" instead\n",
      "  warnings.warn('DeprecationWarning: pretrained is deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: ./ckpt/working/bevdet-r50-depth-cbgs.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jeholee/anaconda3/envs/omnicv/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "\"\"\" \n",
    "2023-3-23 개발 노트\n",
    "- BEVDepth 논문 다시 보면서 알게된 사실\n",
    "    - BEVDet은 BEVDepth의 여러 모듈을 적용\n",
    "    - 하지만, 오직 bevdet4d-r50-depth-cbgs.py에서만 적용됨\n",
    "    - bevdet-r50 즉 image-based detector에서는 적용되지 않음\n",
    "\"\"\"\n",
    "# config = \"./configs/bevdet/bevdet4d-r50-depth-cbgs.py\"\n",
    "# ckpt = \"./ckpt/bevdet4d-r50-depth-cbgs.pth\"\n",
    "\n",
    "# config = \"./configs/bevdet/bevdet-r50-cbgs.py\"\n",
    "# ckpt = \"./ckpt/bevdet-r50-cbgs.pth\"\n",
    "\n",
    "config = \"./configs/bevdet/bevdet-r50-depth-cbgs.py\"\n",
    "ckpt = \"./ckpt/working/bevdet-r50-depth-cbgs.pth\" # 2023-3-28 trained by jeho\n",
    "\n",
    "\"\"\" Init configuration \"\"\"\n",
    "cfg = Config.fromfile(config)\n",
    "cfg = compat_cfg(cfg)\n",
    "\n",
    "if cfg.get('cudnn_benchmark', False): # set cudnn_benchmark\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\"\"\" Init BEVDet model \"\"\"\n",
    "cfg.model.pretrained = None\n",
    "distributed = False\n",
    "if '4D' in cfg.model.type: # video-based or not\n",
    "    cfg.model.align_after_view_transfromation = True\n",
    "cfg.model.train_cfg = None\n",
    "\n",
    "cfg.model.img_view_transformer.accelerate = True\n",
    "\n",
    "model = build_model(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "fp16_cfg = cfg.get('fp16', None)\n",
    "if fp16_cfg is not None:\n",
    "    wrap_fp16_model(model)\n",
    "checkpoint = load_checkpoint(model, ckpt, map_location=device) # or CPU?\n",
    "model.CLASSES = cfg.class_names\n",
    "\n",
    "\"\"\" Models to GPU memory \"\"\"\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "\"\"\" Init Tangent Projection Grid \"\"\"\n",
    "num_rows = 1\n",
    "num_cols = [6]\n",
    "phi_centers = [0]\n",
    "\n",
    "# fov 가로/세로 비율과 nuscenes input의 width/height 비율이 같음\n",
    "# 900/1600, 396/704: 1.777777...\n",
    "# 위 비율에 맞춰서 tangent patch size 결정하기\n",
    "# 중요! 704, 256은 aspect ratio가 다름. 원래 코드에서도 704, 396으로 resize하고 추후에 704, 256으로 crop함\n",
    "tangent_h = 396 # 900 # 396\n",
    "tangent_w = 704 # 1600 # 704\n",
    "\n",
    "fov  = [70, 39.375]\n",
    "\n",
    "erp_h, erp_w = 1920, 3840\n",
    "\n",
    "n_patch, grid = createProjectGrid(erp_h, erp_w, tangent_h, tangent_w, num_rows, num_cols, phi_centers, fov)\n",
    "grid = grid.to(device)\n",
    "\n",
    "vis_tangent_h = 900 # visualization resolution\n",
    "vis_tangent_w = 1600\n",
    "\n",
    "n_patch, vis_grid = createProjectGrid(erp_h, erp_w, vis_tangent_h, vis_tangent_w, num_rows, num_cols, phi_centers, fov)\n",
    "vis_grid = vis_grid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Init dataset, data loader, infos \"\"\"\n",
    "cfg.data.test.test_mode = True\n",
    "if cfg.data.test_dataloader.get('samples_per_gpu', 1) > 1:\n",
    "    # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "    cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n",
    "\n",
    "test_dataloader_default_args = dict(samples_per_gpu=1, workers_per_gpu=1, dist=distributed, shuffle=False)\n",
    "test_loader_cfg = {\n",
    "        **test_dataloader_default_args,\n",
    "        **cfg.data.get('test_dataloader', {})\n",
    "    }\n",
    "\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(dataset, **test_loader_cfg)\n",
    "\n",
    "infos = dataset.data_infos\n",
    "\n",
    "data_iterator = iter(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jeholee/omni3D/BEVDeTrack/mmdet3d/datasets/pipelines/loading.py:1127: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n",
      "/data/home/jeholee/omni3D/BEVDeTrack/mmdet3d/datasets/pipelines/loading.py:1127: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n",
      "/data/home/jeholee/omni3D/BEVDeTrack/mmdet3d/datasets/pipelines/loading.py:1127: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n",
      "/data/home/jeholee/omni3D/BEVDeTrack/mmdet3d/datasets/pipelines/loading.py:1127: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)\n"
     ]
    }
   ],
   "source": [
    "data = next(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 3, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['img_inputs'][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['img_inputs'][0][0] = data['img_inputs'][0][0].to(device)\n",
    "data['img_inputs'][0][1] = data['img_inputs'][0][1].to(device)\n",
    "data['img_inputs'][0][2] = data['img_inputs'][0][2].to(device)\n",
    "data['img_inputs'][0][3] = data['img_inputs'][0][3].to(device)\n",
    "data['img_inputs'][0][4] = data['img_inputs'][0][4].to(device)\n",
    "data['img_inputs'][0][5] = data['img_inputs'][0][5].to(device)\n",
    "data['img_inputs'][0][6] = data['img_inputs'][0][6].to(device)\n",
    "\n",
    "data['img_metas'][0] = data['img_metas'][0].data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jeholee/omni3D/BEVDeTrack/mmdet3d/core/bbox/coders/centerpoint_bbox_coders.py:201: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.post_center_range = torch.tensor(\n",
      "/data/home/jeholee/omni3D/BEVDeTrack/mmdet3d/core/bbox/coders/centerpoint_bbox_coders.py:201: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.post_center_range = torch.tensor(\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    result = model(return_loss=False, rescale=True, **data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnicv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
